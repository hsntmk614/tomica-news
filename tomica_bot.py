import requests
from bs4 import BeautifulSoup
import json
import os

# --- è¨­å®šéƒ¨åˆ† ---
# GitHubã®é‡‘åº«ã‹ã‚‰LINEã®éµã‚’è‡ªå‹•ã§å¼•ãå‡ºã—ã¾ã™
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN", "")
LINE_USER_ID = os.environ.get("LINE_USER_ID", "")

# ç›£è¦–ã—ãŸã„ã‚µã‚¤ãƒˆã®URLãƒªã‚¹ãƒˆï¼ˆä¾‹ï¼šã‚¿ã‚«ãƒ©ãƒˆãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ«ï¼‰
TARGET_URLS = [
    "https://takaratomymall.jp/shop/c/cTomica/",
]

# éå»ã«é€šçŸ¥ã—ãŸæƒ…å ±ã‚’è¨˜éŒ²ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«
HISTORY_FILE = "tomica_history.json"

def send_line_message(message_text):
    """LINEã«é€šçŸ¥ã‚’é€ã‚‹é–¢æ•° (æ–°ã—ã„Messaging APIç‰ˆ)"""
    if not LINE_CHANNEL_ACCESS_TOKEN or not LINE_USER_ID:
        print("LINEã®éµãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
        
    endpoint = "https://api.line.me/v2/bot/message/push"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"
    }
    payload = {
        "to": LINE_USER_ID,
        "messages": [
            {
                "type": "text",
                "text": message_text
            }
        ]
    }
    
    response = requests.post(endpoint, headers=headers, json=payload)
    if response.status_code != 200:
        print(f"LINEé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {response.text}")

def load_history():
    """éå»ã®é€šçŸ¥å±¥æ­´ã‚’èª­ã¿è¾¼ã‚€"""
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_history(history):
    """é€šçŸ¥å±¥æ­´ã‚’ä¿å­˜ã™ã‚‹"""
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(history, f, ensure_ascii=False, indent=4)

def check_new_tomica():
    """ã‚µã‚¤ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ–°ç€ãŒã‚ã‚Œã°é€šçŸ¥ã™ã‚‹ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ãƒˆãƒŸã‚«ã®æœ€æ–°æƒ…å ±ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
    history = load_history()
    new_items_found = False

    for url in TARGET_URLS:
        try:
            headers = {'User-Agent': 'Mozilla/5.0'} 
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            links = soup.find_all('a')
            
            for link in links:
                title = link.text.strip()
                href = link.get('href')
                
                if "ãƒˆãƒŸã‚«" in title and href:
                    if href.startswith('/'):
                        domain = "/".join(url.split("/")[:3])
                        full_url = domain + href
                    else:
                        full_url = href

                    item_id = full_url 
                    
                    if item_id not in history:
                        message = f"ğŸš— æ–°ç€ãƒˆãƒŸã‚«æƒ…å ±ï¼\n{title}\n{full_url}"
                        print(message)
                        send_line_message(message)
                        
                        history.append(item_id)
                        new_items_found = True
                        
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({url}): {e}")

    if new_items_found:
        save_history(history[-200:])
        print("ãƒã‚§ãƒƒã‚¯å®Œäº†ã€‚æ–°ç€æƒ…å ±ã‚’é€šçŸ¥ã—ã¾ã—ãŸã€‚")
    else:
        print("ãƒã‚§ãƒƒã‚¯å®Œäº†ã€‚æ–°ç€æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

if __name__ == "__main__":
    check_new_tomica()
